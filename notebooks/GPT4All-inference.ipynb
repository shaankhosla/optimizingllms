{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/shaankhosla/optimizingllms/blob/main/notebooks/GPT4All-inference.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Powa-rm3ApS0"
   },
   "source": [
    "# GPT4All CPU Interface\n",
    "\n",
    "In this notebook, we jump as quickly as we can into a command-line interaction with a GPT-4-like model that is on our own device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vVpLf3_fX3qK"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install nomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eegz7O7Nuvj0"
   },
   "outputs": [],
   "source": [
    "from nomic.gpt4all import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZ3Rz0grAneG",
    "outputId": "ce4cfa9f-eb98-43d0-86f4-7f5931e4b944"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-04 07:48:01.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.gpt4all.gpt4all\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mDownloading executable...\u001b[0m\n",
      "51KB [00:00, 709.18KB/s]              \n",
      "\u001b[32m2023-05-04 07:48:01.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.gpt4all.gpt4all\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mDownloading model...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully to /root/.nomic/gpt4all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "514250KB [01:37, 5274.58KB/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully to /root/.nomic/gpt4all-lora-quantized.bin\n"
     ]
    }
   ],
   "source": [
    "m = GPT4All()\n",
    "m.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "H_Fscz0qBAAl",
    "outputId": "9c0408e2-27ad-4de0-c15a-2894e4d6364d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1 A large language model (LLM) refers to an artificial intelligence algorithm that learns from vast amounts of data and can be trained on multiple domains, including natural language processing tasks like machine translation or text classification/retrieval. LLMs are typically based on deep learning techniques such as recurrent neural networks (RNN), which allow them to learn complex patterns in unstructured datasets without being explicitly programmed for each task they perform.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.prompt(\"Please answer this question: What is a Large Language Model?\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPktxGZhE4MIOCn0YzH1ztC",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
