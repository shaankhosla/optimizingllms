{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/shaankhosla/optimizingllms/blob/main/notebooks/Tokenization.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YN46LPl-l54T"
   },
   "source": [
    "# GPT\n",
    "\n",
    "In this notebook (based on [Sinan Ozdemir's](https://github.com/sinanuozdemir/oreilly-gpt-hands-on-nlg/blob/main/notebooks/Introduction_to_GPT.ipynb)), we:\n",
    "\n",
    "1. Use `transformers` pipeline objects to generate text very easily (using a GPT model)\n",
    "2. Explore tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "x4D097ejSDzh"
   },
   "source": [
    "### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67O5gEnnB4V3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install transformers==4.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSEmQMy09mG4"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, GPT2Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9USHAIspR-lC"
   },
   "source": [
    "### Hello, Pipeline! \n",
    "\n",
    "Let's use the `pipeline` object to generate text.\n",
    "\n",
    "Other examples of tasks we can carry out with pipelines include:\n",
    "* `\"sentiment-analysis\"`\n",
    "* `\"ner\"` (named entity recognition)\n",
    "* `\"summarization\"`\n",
    "* `\"translation_en_to_fr\"`\n",
    "* `\"feature-extraction\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aEB3MyurB1Ug",
    "outputId": "a76dfd9e-de01-4cd1-80be-5cb02254a679"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The capital of Germany is Berlin. The capital of China is Beijing. The capital of France is Paris.'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "generator(\n",
    "    \"The capital of Germany is Berlin. The capital of China is Beijing. The capital of France is\",\n",
    "    max_new_tokens=2,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YqEJxtLiSZz7"
   },
   "source": [
    "### Exploring tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgU8F2s6CfJz"
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  # load up a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ixSB2OeESmfd",
    "outputId": "d6be03a6-641c-48a3-98c6-edc49a733b5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"love\" in tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQGMZ-9wSjvg",
    "outputId": "a6e6725b-3a6d-4c4e-fa9d-8b4ae124b7c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Sinan\" in tokenizer.get_vocab()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ok6nimr0XWKw"
   },
   "source": [
    "Encode a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgHBvaBWnNFI",
    "outputId": "c4ffc510-fb43-4610-952e-cdd5dccb162d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46200, 272, 10408, 257, 4950, 1110]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Sinan loves a beautiful day\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0wVJdiacXcLv"
   },
   "source": [
    "...then convert the ids into tokens: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CLWaENSZnWVQ",
    "outputId": "a4f0aedf-9c5f-40ca-e0b7-1496293451a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sin', 'an', 'Ġloves', 'Ġa', 'Ġbeautiful', 'Ġday']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode(\"Sinan loves a beautiful day\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oZYln4urXPkF"
   },
   "source": [
    "(The `Ġ` character denotes a space before the token.)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMe8TfdiLEzivRvXtS3IiEh",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
